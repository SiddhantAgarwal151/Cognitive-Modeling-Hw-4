{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0494002f",
   "metadata": {},
   "source": [
    "Problem 1:\n",
    "I'll mark each statement as true or false:\n",
    "\n",
    "1. Direct K-fold cross-validation requires K model re-fits, which may be computationally demanding, especially when inverse inference is costly. **TRUE**\n",
    "\n",
    "2. Bayes factors (BFs) are relative measures, that is, they cannot differentiate between \"equally good\" and \"equally bad\" models. **TRUE**\n",
    "\n",
    "3. Marginal likelihoods and, by extension, Bayes factors (BFs) cannot be used to compare models with different likelihoods. **FALSE** - Bayes factors can compare models with different likelihoods as they are ratios of marginal likelihoods.\n",
    "\n",
    "4. Both the Binomial and the Dirichlet distribution can be formulated as special cases of the Multinomial distribution. **FALSE** - While the Binomial is a special case of the Multinomial, the Dirichlet is not. The Dirichlet is a distribution over probability vectors, while the Multinomial is a distribution over counts.\n",
    "\n",
    "5. Bayesian leave-one-out cross-validation (LOO-CV) relies on the posterior predictive distribution of left-out data points. **TRUE**\n",
    "\n",
    "6. The Akaike Information Criterion (AIC) penalizes model complexity indirectly through the variance of a model's marginal likelihood. **FALSE** - AIC penalizes model complexity directly through the number of parameters, not through variance of the marginal likelihood.\n",
    "\n",
    "7. The log-predictive density (LPD) is a relative metric of model complexity. **FALSE** - LPD is a measure of predictive accuracy, not complexity.\n",
    "\n",
    "8. The LPD can be approximated by evaluating the likelihood of each posterior draw (e.g., as provided by an MCMC sampler) and taking the average of all resulting likelihood values. **TRUE**\n",
    "\n",
    "9. Bayes factors do not depend on the prior odds, that is, the ratio of prior model probabilities p(M1)/p(M2). **TRUE**\n",
    "\n",
    "10. we should always prefer information criteria to cross-validation in terms of estimation predictive performance. **FALSE** - This blanket statement is incorrect. The best method depends on the specific context, with cross-validation often being more accurate but computationally expensive."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
